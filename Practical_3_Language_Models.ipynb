{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-590a9129-c91d-4964-876b-cea5893ef2f0",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "# Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-bda1c2d6-734c-4117-9e7d-649081f43214",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## What is a language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-36ad7e16-fe27-41b4-a814-f857b543affd",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "A **language model** learns to predict the probability of a sequence of words. \n",
    "Why do we need to learn the probability of words? \n",
    "\n",
    "Which sentence is more probable: \n",
    "\"The small is cat\" or \"The cat is small\"? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-59c416aa-cd4b-40ea-b16b-53248d31d927",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Why do we need language models?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-3c72010c-b869-46bf-8918-11fc8b28e77c",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "- Machine Translation\n",
    "\n",
    "        Les Grandes Espérances \n",
    "            - Great Expectations\n",
    "            - Tall Expectations\n",
    "\n",
    "\n",
    "- Spell Correction\n",
    "\n",
    "        Choose most probable acccording to context matching a language model\n",
    "\n",
    "          - I love movies directed by Lynch.\n",
    "          - I love movies directed by Lunch.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/Lunch.png\" width =\"600\">\n",
    "\n",
    "\n",
    "- Speech Recognition\n",
    "      - Choose the most probable utterence among ones that are phonetically similar\n",
    "     \n",
    "          Say both of these out fast and loudly:\n",
    "\n",
    "          - wreck a nice beach\n",
    "          - reckon ice beach\n",
    "          - recognize speech\n",
    "\n",
    "          Which one is more probable to occur in English?\n",
    "     \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-47725df5-cc38-4302-b99e-570d4435a387",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Types of language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-39c60ab7-a553-4470-880d-71830905617f",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "- **Statistical Language Models**\n",
    "(traditional statistical techniques like N-grams, Hidden Markov Models (HMM) and certain linguistic rules to learn the probability distribution of words)\n",
    "\n",
    "- **Neural Language Models**\n",
    "(new and more efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-b8c978ee-dbd0-417d-afad-5db4858f125e",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## N-Gram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-9a7ac1cf-603c-4fde-9d54-c763b9f6ab92",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Let’s understand N-grams with an example. Consider the following sentence:\n",
    "\n",
    "“I enjoy reading books about data science”\n",
    "\n",
    "A **1-gram (or unigram)** is a one-word sequence. For the above sentence, the unigrams would simply be: “I”, \"enjoy\", “reading”, \"books, “about”, “data”, “science”.\n",
    "\n",
    "A **2-gram (or bigram)** is a two-word sequence of words, like “I enjoy\", \"enjoy reading\". \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-2a02a525-09eb-4d56-a371-95b6c59c98cf",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "1 - **REFLECT AND REPLY**: What is a **3-gram (or trigram)**? Please give an example from the sentence above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write you answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-a01a3033-69f4-4f99-a423-806d24082fe8",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "An N-gram language model predicts the probability of a given N-gram occuring within any sequence of words in the language. If we have a quality N-gram model, we can predict probability **p(w | h)** – what is the probability of seeing the word **w** given a history of previous words **h** – where the history contains n-1 words.\n",
    "\n",
    "To build an N-Gram model we have to estimate this probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-2d77afc0-6fc8-4d34-bb89-858922c75153",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "2 - **REFLECT AND REPLY:** How many n-grams are in a sentence with **m** number of unigrams? (n>1)\n",
    "\n",
    "### Write your anwser here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-7def682c-4f90-4f32-bcb6-e633149a230a",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "**If you Fancy** Watch [this video series](https://youtu.be/Saq1QagC8KY) by Dan Jurafsky on Language Modeling (4.1- 4.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-66e5087b-94eb-425b-88ce-dacf96caa717",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Google NGRAMS \n",
    "\n",
    "Google ngrams is a tool which looks for ngrams in resources from google and visualizes them on a timeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-289f2f19-f689-4a91-be95-772197a58849",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "https://books.google.com/ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-9bd06e3c-5542-41cb-8c5f-cc596368ce16",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Markov chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-982fbc0e-0408-495f-b3bb-0e49f80571c7",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "How do we compute the probability?\n",
    "\n",
    "1. Apply the chain rule of probability\n",
    "2. Apply a very strong simplification assumption to allow us to compute $p(w1…ws)$ easily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-d9faa5c7-0dfd-4541-b687-7acea0976230",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "The assumption that the probability of a word depends only on the previous word\n",
    "Markov is called a **Markov assumption**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-e975829b-2aa2-436d-848b-d274faed929e",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "In a Markov chain:\n",
    "-  each event in the sequence comes from a set of outcomes that depend on one another;\n",
    "- each outcome determines which outcomes are likely to occur next;\n",
    "- the most recent event contain all the information we need to predict the next event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-4a223996-975c-4bae-8673-3a9f132a1aa5",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "When we talk about language it means that:\n",
    "- each word in a sentence comes from a set of words each depending on one another;\n",
    "- each word determines which word is likely to be the next one in a sentence;\n",
    "- the most recent word contains all the information to allow us predict the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-d00aaeb7-2d05-477d-aa76-46ae90e86b91",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "### Markov Assumption for Bigram language model:\n",
    "<img src=\"img/bigram.png\" width =\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-138ff77d-5221-4699-ab7c-e34713cd2e32",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Let's do an exercise:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00016-6fab1e76-d9be-47d1-9695-e823b6ed7598",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "print(nltk.corpus.gutenberg.fileids())\n",
    "text = nltk.corpus.gutenberg.open(\"shakespeare-hamlet.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-dc97799a-9814-4da8-aa22-8e7de48a434f",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-5b12843c-a9c8-4e3e-9a14-45e43faa2de9",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "3 - **CODEIT** Do you remember from first session which NLTK function we can uyse instead of splits to extrat tokens from text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00024-33a35897-a5db-4789-99e7-59643825ec5b",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #run this line if you haven't before\n",
    "# Insert the function to tokenize the text\n",
    "corpus = nltk.tokenize.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-bbc99227-489d-4dde-9edb-4e16a03047df",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Let's get word pairs (to avoid filling the memory we will use a generator that yields word pairs for further processing without storing them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00018-49517c4a-c0fc-4f27-9e19-bb910045604f",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_pairs(corpus):\n",
    "    for i in range(len(corpus)-1):\n",
    "        yield (corpus[i], corpus[i+1])\n",
    "        \n",
    "pairs = make_pairs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-65281a75-be3e-467e-ae2c-0398190cabc0",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairs\n",
    "#now pairs is a bigram generator that creates the next bigram each time it's called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-8fc610dd-2ed6-41c2-9706-c94dc4892c87",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Let's now create an empty dictionary and fill it with out word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00022-26850c90-c28c-4540-a71c-0660ce673555",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for word_1, word_2 in pairs:\n",
    "    if word_1 in word_dict.keys():\n",
    "        word_dict[word_1].append(word_2)\n",
    "    else:\n",
    "        word_dict[word_1] = [word_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00034-c8b0e432-ba64-40e0-984d-82d2af0fd3f4",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_dict[\"King\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-a9509faf-9434-4f5e-9808-8f172aba58aa",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Now let's choose a random word to start off the chain, and specify the number of words the chain will simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-2b361d79-8363-4694-9cf5-73361e4ddfd7",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "first_word = np.random.choice(corpus)\n",
    "chain = [first_word]\n",
    "n_words = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-22e5c1e3-a3fe-4970-8fa7-11250dec7aae",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "As for the next words, they will be sampled randomly from the list of words which followed that first word in the texts we have supplied:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00027-108a69ac-9ebc-49bf-ae07-026287e15370",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_words):\n",
    "    chain.append(np.random.choice(word_dict[chain[-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-e45f8479-c67f-4400-9398-f84fb04a2ce6",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Last step: print out the generated text! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00028-ec0799e4-6e0c-4013-b976-a034434f5489",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "generated_text = ' '.join(chain)\n",
    "print (generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-83ef249b-b631-41ef-87f5-b3409374c57b",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Limitations of the n-gram model:\n",
    "\n",
    "- The higher the N, the better is the model - but this leads to lots of computation overhead that requires large computation power in terms of RAM;\n",
    "- N-grams are a sparse representation of language. This is because we build the model based on the probability of words co-occurring. It will give zero probability to all the words that are not present in the training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-2adaf340-8986-4f14-86c9-607a60c9ac5e",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Train an Ngram Langugae model using nltk.lm library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00041-82ac0782-c5b8-4063-bee2-61adb597253d",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "The following code loads the text of inaugural speech of Donald Trump from the nltk corpuses library and bilds a 3-gram language model by Maximizing Likelihood Estimation of the train text.\n",
    "\n",
    "It adds start and end of sentences to the begining and end of sentences as tokens and then creates all n-grams $n<=3$\n",
    "\n",
    "We kept 5 percent of the sentences in the corpus for computing **perplexity** to evaluate our language model intrinsically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00030-5056aa41-3391-4e3e-a4c8-94fec73cb8b7",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from numpy import random\n",
    "n = 3\n",
    "from nltk.corpus import inaugural\n",
    "nltk.download('inaugural')\n",
    "\n",
    "text = nltk.corpus.inaugural.open(\"2017-Trump.txt\").read()\n",
    "\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "random.shuffle(tokenized_text)\n",
    "# separate into training and test\n",
    "tokenized_text_train = tokenized_text[0:int(0.95*(len(tokenized_text)))]\n",
    "tokenized_text_test = tokenized_text[int(0.95*(len(tokenized_text)))+1:]\n",
    "\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text_train)\n",
    "\n",
    "from nltk.lm import MLE\n",
    "model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00043-d84d04cb-a891-49fa-9e7d-fa09c11334d8",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "4 - **REFLECT AND REPLY:** \n",
    "\n",
    "Could you give any explanations why the text was shuffled before taking samples for train and test sets?(Hint: take a look at the text)\n",
    "\n",
    "\n",
    "This text source is quite short, at the end of the speech there are several sentences which the speaker is saying thanks. Which means that the last sentences are all of same pattern and the language model can not see  them in the training data.  So it's better to shuffle the sentences so that the training data contains patterns whether they are at the beginining or end of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00044-f35976ec-e880-48bd-97ee-3ac4fc7886e8",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Write your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00043-99f6dc48-c8f5-4bce-b0ac-ff7f0c2fdbef",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-48288294-5a24-4319-9a63-c522b4f3386e",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-9d38f1c8-dcb9-4450-86aa-e7be624b128b",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "The following code get the counts of n-grams from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-1b9c28d0-caef-420b-938c-4600fe3628b2",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.counts['united']) # i.e. Count('humbly')\n",
    "model.counts[['united']]['states'] # i.e. Count('humbly'|'most')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-5b42ae4b-f1ae-4fd9-a32b-8b5127904247",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "5 - **CODEIT** Print the count of a trigram which might occur/not in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00047-e92a72a3-6462-4396-a801-8a70317cde21",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Insert Code here \n",
    "model.counts[['united','states']][\"of\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00050-c7531cc2-dee3-4017-a7d3-b828a57ddf17",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "6 - **CODEIT** The character sequence which represents the start of the sentence is **<s>** in nltk language model library.\n",
    "\n",
    "Write a code to print the score how many times the pronoun \"I\" in the corpus occurs in the begining of the sentence.\n",
    "\n",
    "**NOTE**: the text is lower-cased in the preprocessing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00050-0aaa770b-a61e-42d7-a428-1d684786fbc7",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Insert code here\n",
    "\n",
    "print(model.score('i',['<s>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00049-2d3d57b2-7798-4b90-bf0d-7a4947e0007d",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.score('strong', [\"make\" ,\"america\"]))\n",
    "print(model.score('states', [\"united\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00051-3cc24f4e-3a01-4396-9b7a-b8c5f1e6f6ed",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "7 - **REFLECT AND REPLY** \n",
    "\n",
    "Write the  probablity that the two statements above represent in terms of $P(W_1|W_2,...)$\n",
    "\n",
    "### write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00051-10b417c1-4097-47d0-9f1e-ec9483688ddd",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "The following function generates a sentence based on your language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00052-d4e4eae6-59c0-4b84-8df5-217a64f42375",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import random\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words,text_seed=\"\"):\n",
    "    content = []\n",
    "    if(text_seed==\"\"):\n",
    "        text_seed = \"<s>\"\n",
    "        \n",
    "    else:\n",
    "        content.append(text_seed)\n",
    "    \n",
    "    for token in model.generate(num_words,text_seed= text_seed,random_seed=random.Random()):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "        \n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00053-337a448c-df11-421b-8f26-daa26a62fdfd",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_sent(model,10,\"i\")\n",
    "print(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00058-a3598005-70c2-4507-99a5-6ce444f82fa2",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data, padded_sents_test = padded_everygram_pipeline(n, tokenized_text_test)\n",
    "for i,test in enumerate(test_data):\n",
    "    print(\"Perplexity for <\",\" \".join(tokenized_text_test[i]),\"> is: \", model.perplexity(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00061-18c66253-d083-4346-9732-96767716b11c",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "8 - ** Reflect and Reply**  What is the perplexity value for the  language model on the test sentences ? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00062-c50f6d7a-d6aa-4e5f-8f4f-20c8d99127c1",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00063-c577d94a-dca9-4905-a418-414819cf170c",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "9 - **CODEIT** Instead of Maximum likelihood estimation implemented in nltk.lm.MLE train a language model using the Laplace model class implemented in the nltk.lm.Laplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00061-bfd7db12-21da-4eb4-9ccc-66d9fe340a54",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from numpy import random\n",
    "n = 3\n",
    "from nltk.corpus import inaugural\n",
    "nltk.download('inaugural')\n",
    "\n",
    "text = nltk.corpus.inaugural.open(\"2017-Trump.txt\").read()\n",
    "\n",
    "\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "random.shuffle(tokenized_text)\n",
    "# separate into training and test\n",
    "tokenized_text_train = tokenized_text[0:int(0.95*(len(tokenized_text)))]\n",
    "tokenized_text_test = tokenized_text[int(0.95*(len(tokenized_text)))+1:]\n",
    "\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text_train)\n",
    "\n",
    "from nltk.lm import Laplace\n",
    "model = Laplace(n) # Lets train a 3-grams model, previously we set n=3\n",
    "model.fit(train_data, padded_sents)\n",
    "\n",
    "\n",
    "test_data, padded_sents_test = padded_everygram_pipeline(n, tokenized_text_test)\n",
    "for i,test in enumerate(test_data):\n",
    "    print(\"Perplexity for <\",\" \".join(tokenized_text_test[i]),\"> is: \", model.perplexity(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00065-7d3eb37b-136d-49e1-bb77-b2898857862d",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "10 - **Reflect and Reply** Print the perplexities for the test sentences and reflect on the way they are differenet from the perplexity of MLE model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00066-678c5e4f-39ca-4fa9-b461-d3ca6880ef15",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "### Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-203a39f3-75c4-429a-b171-94b788e45d7d",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Neural Language Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00057-3f67176b-bbbe-491f-8197-79dfe055d8c4",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "**GPT-2**, is an unsupervised transformer language model and the successor to GPT. GPT-2 was first announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released out of concern over potential misuse, including applications for writing fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00058-d6974a34-d028-488a-84c9-6d1aa465df6c",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "**GPT 3**: https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/\n",
    "\n",
    "The following article was written by a GPT 3 model (Impressive or meh?!): https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00070-d986e30f-1e5f-41f3-b042-56bb5616c1c7",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "11 - **Homework** The previous models used n=3 , i.e. ngrams where  $n<=3$. Train a Laplace language model with ngrams where $n<=4$. Then compare the perplexity of the model and write your reflection on the difference between perplexities.\n",
    "\n",
    "\n",
    "The intuition is that when higher n-grams are considered the language model should improve and thus perplexity should be lower. However when higher ngrams are considered, the risk of overfitting also increasing , means that the model will learn very specific sequences in the training data and lose the ability to generalize\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "*** If you fancy *** you can train you language models for any text in any language.\n",
    "\n",
    "However make sure you same the same resource for both models you are comparing the perplexity for."
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "fdb5c6ab-36b4-4c08-b3f2-5e05b24bd7ca",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
